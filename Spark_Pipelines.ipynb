{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n30FHl2FvePu",
        "outputId": "817d007a-b6e3-4da5-ef8c-871e34d2765b"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 83kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 22.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=9f6c60e37dd0ce88c1fbd88c06f615e56e6e718d7153c041ebcb74f19a7d0d48\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7NPH80_vMki"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.classification import LogisticRegression\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgWVF-7Bvqkh"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('abc').getOrCreate()\n",
        "# Chuẩn bị dữ liệu đào tạo từ danh sách các bộ giá trị (nhãn, tính năng).\n",
        "training = spark.createDataFrame([\n",
        "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
        "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
        "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
        "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-xwxdpFzVMe",
        "outputId": "c48f16d3-3f83-4a1e-f25c-d1e95a1e8f76"
      },
      "source": [
        "training.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------+\n",
            "|label|      features|\n",
            "+-----+--------------+\n",
            "|  1.0| [0.0,1.1,0.1]|\n",
            "|  0.0|[2.0,1.0,-1.0]|\n",
            "|  0.0| [2.0,1.3,1.0]|\n",
            "|  1.0|[0.0,1.2,-0.5]|\n",
            "+-----+--------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3C01hIexDtU"
      },
      "source": [
        "# Tạo một thể hiện LogisticRegression. Ví dụ này là một Công cụ ước tính.\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-dTl1_jxKlp",
        "outputId": "fcf58910-efec-4cba-cff3-142ebceb5672"
      },
      "source": [
        "# In ra các thông số, tài liệu và bất kỳ giá trị mặc định nào\n",
        "print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression parameters:\n",
            "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
            "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
            "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
            "featuresCol: features column name. (default: features)\n",
            "fitIntercept: whether to fit an intercept term. (default: True)\n",
            "labelCol: label column name. (default: label)\n",
            "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
            "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
            "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
            "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
            "predictionCol: prediction column name. (default: prediction)\n",
            "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
            "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
            "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.01)\n",
            "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
            "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
            "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
            "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
            "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
            "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
            "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZcP6MGxgHw"
      },
      "source": [
        "# Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
        "model1 = lr.fit(training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9heirw7xiBg",
        "outputId": "bf9067f3-9901-42f8-dc9e-32e0e7f2d3cb"
      },
      "source": [
        "# Vì model1 là Mô hình (tức là máy biến áp do Công cụ ước tính sản xuất),\n",
        "# chúng ta có thể xem các tham số mà nó sử dụng trong fit ().\n",
        "# Điều này in ra các cặp tham số (name: value), trong đó tên là ID duy nhất cho điều này\n",
        "# Phiên bản LogisticRegression.\n",
        "print(\"Model 1 was fit using parameters: \")\n",
        "print(model1.extractParamMap())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model 1 was fit using parameters: \n",
            "{Param(parent='LogisticRegression_8843034b63dd', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2, Param(parent='LogisticRegression_8843034b63dd', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0, Param(parent='LogisticRegression_8843034b63dd', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto', Param(parent='LogisticRegression_8843034b63dd', name='featuresCol', doc='features column name.'): 'features', Param(parent='LogisticRegression_8843034b63dd', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LogisticRegression_8843034b63dd', name='labelCol', doc='label column name.'): 'label', Param(parent='LogisticRegression_8843034b63dd', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0, Param(parent='LogisticRegression_8843034b63dd', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='LogisticRegression_8843034b63dd', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LogisticRegression_8843034b63dd', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability', Param(parent='LogisticRegression_8843034b63dd', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='LogisticRegression_8843034b63dd', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent='LogisticRegression_8843034b63dd', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LogisticRegression_8843034b63dd', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5, Param(parent='LogisticRegression_8843034b63dd', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSF2LbwTx2oO"
      },
      "source": [
        " #Theo cách khác, chúng tôi có thể chỉ định các tham số bằng cách sử dụng từ điển Python dưới dạng paramMap\n",
        "paramMap = {lr.maxIter: 20}\n",
        "paramMap[lr.maxIter] = 30  #Chỉ định 1 Tham số, ghi đè lên maxIter ban đầu.\n",
        "#Chỉ định nhiều Tham số.\n",
        "paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55})  # type: ignore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg9gMQzLyTjc"
      },
      "source": [
        "# Bạn có thể kết hợp paramMaps, là từ điển python.\n",
        "# Thay đổi tên cột đầu ra\n",
        "paramMap2 = {lr.probabilityCol: \"myProbability\"}  # type: ignore\n",
        "paramMapCombined = paramMap.copy()\n",
        "paramMapCombined.update(paramMap2)  # type: ignore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCPbvmgeydW5",
        "outputId": "6144ef6f-bd27-4adc-c304-9eb24182e787"
      },
      "source": [
        "# Bây giờ tìm hiểu một mô hình mới bằng cách sử dụng các tham số paramMapCombined.\n",
        "# paramMapCombined ghi đè tất cả các tham số đã đặt trước đó thông qua các phương thức lr.set *\n",
        "model2 = lr.fit(training, paramMapCombined)\n",
        "print(\"Model 2 was fit using parameters: \")\n",
        "print(model2.extractParamMap())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model 2 was fit using parameters: \n",
            "{Param(parent='LogisticRegression_8843034b63dd', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2, Param(parent='LogisticRegression_8843034b63dd', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0, Param(parent='LogisticRegression_8843034b63dd', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto', Param(parent='LogisticRegression_8843034b63dd', name='featuresCol', doc='features column name.'): 'features', Param(parent='LogisticRegression_8843034b63dd', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LogisticRegression_8843034b63dd', name='labelCol', doc='label column name.'): 'label', Param(parent='LogisticRegression_8843034b63dd', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0, Param(parent='LogisticRegression_8843034b63dd', name='maxIter', doc='max number of iterations (>= 0).'): 30, Param(parent='LogisticRegression_8843034b63dd', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LogisticRegression_8843034b63dd', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'myProbability', Param(parent='LogisticRegression_8843034b63dd', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='LogisticRegression_8843034b63dd', name='regParam', doc='regularization parameter (>= 0).'): 0.1, Param(parent='LogisticRegression_8843034b63dd', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LogisticRegression_8843034b63dd', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.55, Param(parent='LogisticRegression_8843034b63dd', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_1vkfMZy8TU"
      },
      "source": [
        "# Prepare test data\n",
        "test = spark.createDataFrame([\n",
        "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
        "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
        "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfeUCll4zDDJ",
        "outputId": "277d6dc3-2f99-4d71-cac2-7d72a0f079df"
      },
      "source": [
        "test.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------+\n",
            "|label|      features|\n",
            "+-----+--------------+\n",
            "|  1.0|[-1.0,1.5,1.3]|\n",
            "|  0.0|[3.0,2.0,-0.1]|\n",
            "|  1.0|[0.0,2.2,-1.5]|\n",
            "+-----+--------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9y1grgBZzhxd",
        "outputId": "f7b6a648-3137-4a49-816a-5de4788cdeee"
      },
      "source": [
        "# Đưa ra dự đoán về dữ liệu thử nghiệm bằng phương thức Transformer.transform ().\n",
        "# LogisticRegression.transform sẽ chỉ sử dụng cột 'tính năng'.\n",
        "# Lưu ý rằng model2.transform () xuất ra cột \"myProbability\" thay vì cột thông thường\n",
        "# cột 'xác suất' vì trước đây chúng tôi đã đổi tên thông số lr.probabilityCol.\n",
        "prediction = model2.transform(test)\n",
        "result = prediction.select(\"features\", \"label\", \"myProbability\", \"prediction\") \\\n",
        "    .collect()\n",
        "\n",
        "for row in result:\n",
        "    print(\"features=%s, label=%s -> prob=%s, prediction=%s\"\n",
        "          % (row.features, row.label, row.myProbability, row.prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features=[-1.0,1.5,1.3], label=1.0 -> prob=[0.05707304171034107,0.9429269582896589], prediction=1.0\n",
            "features=[3.0,2.0,-0.1], label=0.0 -> prob=[0.9238522311704136,0.0761477688295864], prediction=0.0\n",
            "features=[0.0,2.2,-1.5], label=1.0 -> prob=[0.10972776114780078,0.8902722388521992], prediction=1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2rPfdau3JOt"
      },
      "source": [
        "----------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEvWtUCj3QfE"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO3FmTWn3Xpc"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAC51O123cZ4",
        "outputId": "67274a46-f6b8-48ea-9b7e-8d00cfae95d8"
      },
      "source": [
        "# Chuẩn bị tài liệu đào tạo từ danh sách các bộ giá trị (id, văn bản, nhãn)\n",
        "training = spark.createDataFrame([\n",
        "    (0, \"a b c d e spark\", 1.0),\n",
        "    (1, \"b d\", 0.0),\n",
        "    (2, \"spark f g h\", 1.0),\n",
        "    (3, \"hadoop mapreduce\", 0.0)\n",
        "], [\"id\", \"text\", \"label\"])\n",
        "training.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----------------+-----+\n",
            "| id|            text|label|\n",
            "+---+----------------+-----+\n",
            "|  0| a b c d e spark|  1.0|\n",
            "|  1|             b d|  0.0|\n",
            "|  2|     spark f g h|  1.0|\n",
            "|  3|hadoop mapreduce|  0.0|\n",
            "+---+----------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYyCBmXx3odL"
      },
      "source": [
        "# Định cấu hình đường ống ML, bao gồm ba giai đoạn: tokenizer, hashingTF và lr\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJpeiZJ633nQ"
      },
      "source": [
        "# Điều chỉnh đường ống cho các tài liệu đào tạo.\n",
        "model = pipeline.fit(training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-Axhi123_2n",
        "outputId": "9613d378-c81b-4786-93a9-26b831a87228"
      },
      "source": [
        "# Chuẩn bị các tài liệu thử nghiệm, là các bộ dữ liệu không được gắn nhãn (id, văn bản).\n",
        "test = spark.createDataFrame([\n",
        "    (4, \"spark i j k\"),\n",
        "    (5, \"l m n\"),\n",
        "    (6, \"spark hadoop spark\"),\n",
        "    (7, \"apache hadoop\")\n",
        "], [\"id\", \"text\"])\n",
        "test.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------------+\n",
            "| id|              text|\n",
            "+---+------------------+\n",
            "|  4|       spark i j k|\n",
            "|  5|             l m n|\n",
            "|  6|spark hadoop spark|\n",
            "|  7|     apache hadoop|\n",
            "+---+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oym1KZx24LzA",
        "outputId": "d7e8c436-92f4-4c54-fa9b-cd89e339c280"
      },
      "source": [
        "# Thực hiện dự đoán trên các tài liệu thử nghiệm và in các cột quan tâm.\n",
        "prediction = model.transform(test)\n",
        "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
        "for row in selected.collect():\n",
        "    rid, text, prob, prediction = row  # type: ignore\n",
        "    print(\n",
        "        \"(%d, %s) --> prob=%s, prediction=%f\" % (\n",
        "            rid, text, str(prob), prediction   # type: ignore\n",
        "        )\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, spark i j k) --> prob=[0.1596407738787412,0.8403592261212588], prediction=1.000000\n",
            "(5, l m n) --> prob=[0.8378325685476614,0.16216743145233858], prediction=0.000000\n",
            "(6, spark hadoop spark) --> prob=[0.06926633132976266,0.9307336686702373], prediction=1.000000\n",
            "(7, apache hadoop) --> prob=[0.9821575333444208,0.017842466655579203], prediction=0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEeLpa9G7fj8"
      },
      "source": [
        "# **Phan Loai Hoi Quy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d47eappf7nVP"
      },
      "source": [
        "**Hồi quy logistic nhị thức**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iNO8CYd7jJY"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDzeC4cv7y5e"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDQIOOijFbCj"
      },
      "source": [
        "# **Loại dữ liệu - API dựa trên RDD**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSfOveA0Fch1"
      },
      "source": [
        "**Vectơ địa phương**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz7YbKbYFg5f"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sps\n",
        "from pyspark.mllib.linalg import Vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMjPFpvIFnBP"
      },
      "source": [
        "# Use a NumPy array as a dense vector.\n",
        "dv1 = np.array([1.0, 0.0, 3.0])\n",
        "# Use a Python list as a dense vector.\n",
        "dv2 = [1.0, 0.0, 3.0]\n",
        "# Create a SparseVector.\n",
        "sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])\n",
        "# Use a single-column SciPy csc_matrix as a sparse vector.\n",
        "sv2 = sps.csc_matrix((np.array([1.0, 3.0]), np.array([0, 2]), np.array([0, 2])), shape=(3, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd9o3n_tFyyT",
        "outputId": "6763970d-055f-417c-807a-db3bacb42284"
      },
      "source": [
        "print (dv1)\n",
        "print (dv2)\n",
        "print (sv1)\n",
        "print (sv2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 0. 3.]\n",
            "[1.0, 0.0, 3.0]\n",
            "(3,[0,2],[1.0,3.0])\n",
            "  (0, 0)\t1.0\n",
            "  (2, 0)\t3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOk2J_iUGcpA"
      },
      "source": [
        "# **Điểm được gắn nhãn**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o4IH9-RGeWC"
      },
      "source": [
        "Một điểm được gắn nhãn là một vectơ cục bộ, dày đặc hoặc thưa thớt, được liên kết với một nhãn / phản hồi. Trong MLlib, các điểm được gắn nhãn được sử dụng trong các thuật toán học có giám sát. Chúng tôi sử dụng bộ đôi để lưu trữ một nhãn, vì vậy chúng tôi có thể sử dụng các điểm được gắn nhãn trong cả hồi quy và phân loại. Đối với phân loại nhị phân, nhãn phải là 0(âm) hoặc 1(dương). Đối với phân loại nhiều lớp, nhãn nên chỉ số lớp bắt đầu từ số không: 0, 1, 2, ...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWFMUE0sGhlV"
      },
      "source": [
        "from pyspark.mllib.linalg import SparseVector\n",
        "from pyspark.mllib.regression import LabeledPoint\n",
        "\n",
        "# Create a labeled point with a positive label and a dense feature vector.\n",
        "pos = LabeledPoint(1.0, [1.0, 0.0, 3.0])\n",
        "\n",
        "# Create a labeled point with a negative label and a sparse feature vector.\n",
        "neg = LabeledPoint(0.0, SparseVector(3, [0, 2], [1.0, 3.0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-z_cgfHGmDq",
        "outputId": "1049bfbc-0115-4c9f-c8d0-621c778f5a94"
      },
      "source": [
        "print (pos)\n",
        "print (neg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1.0,[1.0,0.0,3.0])\n",
            "(0.0,(3,[0,2],[1.0,3.0]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr9rRBRyKerA"
      },
      "source": [
        "# **Ma trận cục bộ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ss1UoNXfKhKX",
        "outputId": "fc5caa70-459a-4b5d-ee67-034b1bea7db3"
      },
      "source": [
        "from pyspark.mllib.linalg import Matrix, Matrices\n",
        "\n",
        "# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\n",
        "dm2 = Matrices.dense(3, 2, [1, 3, 5, 2, 4, 6])\n",
        "print (dm2)\n",
        "\n",
        "# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\n",
        "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])\n",
        "print (sm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DenseMatrix([[1., 2.],\n",
            "             [3., 4.],\n",
            "             [5., 6.]])\n",
            "3 X 2 CSCMatrix\n",
            "(0,0) 9.0\n",
            "(2,1) 6.0\n",
            "(1,1) 8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSGRKapkLJzy"
      },
      "source": [
        "# **RowMatrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThoTrKjxLLe5",
        "outputId": "7b43f6e2-8423-40aa-f2c1-802cc2dc5dee"
      },
      "source": [
        "from pyspark.mllib.linalg.distributed import RowMatrix\n",
        "import pyspark\n",
        "sc.stop()\n",
        "# sc = pyspark.SparkContext(\"local\", \"My First Spark App\")\n",
        "\n",
        "sc = pyspark.SparkContext.getOrCreate()\n",
        "# Create an RDD of vectors.\n",
        "rows = sc.parallelize([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
        "\n",
        "print (row)\n",
        "# Create a RowMatrix from an RDD of vectors.\n",
        "mat = RowMatrix(rows)\n",
        "print (mat)\n",
        "\n",
        "# Get its size.\n",
        "m = mat.numRows()  # 4\n",
        "n = mat.numCols()  # 3\n",
        "\n",
        "# Get the rows as an RDD of vectors again.\n",
        "rowsRDD = mat.rows\n",
        "print (rowsRDD)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Row(id=7, text='apache hadoop', probability=DenseVector([0.9822, 0.0178]), prediction=0.0)\n",
            "<pyspark.mllib.linalg.distributed.RowMatrix object at 0x7f15aa65db50>\n",
            "MapPartitionsRDD[3] at mapPartitions at PythonMLLibAPI.scala:1342\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}